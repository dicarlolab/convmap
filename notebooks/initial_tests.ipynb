{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "# sys.path.insert(0, '/Users/pouyabashivan/Dropbox (MIT)/Codes/dldata')\n",
    "import dldata.metrics.utils as utils\n",
    "import dldata.stimulus_sets.hvm as hvm\n",
    "import cPickle\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed([0])\n",
    "from dldata.physiology.hongmajaj.mappings import CHANNEL_INFO\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline\n",
    "sys.path.insert(0, '/braintree/home/bashivan/dropbox/Codes/Dimensionality')\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.misc import imread\n",
    "import re \n",
    "\n",
    "from image_optimizer import *\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "np.random.seed(123)\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "%matplotlib inline\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops       \n",
    "import scipy.stats as stats\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 14, \"ytick.major.size\": 14})\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "\n",
    "dataset_hvm = hvm.HvMWithDiscfade()\n",
    "meta_hvm = dataset_hvm.meta\n",
    "neural_fea = np.array(dataset_hvm.neuronal_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.interpolate import griddata, interp2d\n",
    "\n",
    "\n",
    "def reps_to_array(reps):\n",
    "    \"\"\"\n",
    "    reps: dictionary containing the reps for each variation level  \n",
    "    \"\"\"\n",
    "    max_reps = np.max([reps[i].shape[0] for i in reps.keys()], axis=0)\n",
    "    hvm_neural = np.zeros((max_reps, 5760, reps['V0'].shape[2]))\n",
    "    hvm_neural[...] = np.NaN\n",
    "\n",
    "    c = 0\n",
    "    for key in reps:\n",
    "        shape = reps[key].shape\n",
    "        hvm_neural[:shape[0], c:c+shape[1], :] = reps[key]\n",
    "        c += shape[1]\n",
    "    return hvm_neural\n",
    "\n",
    "\n",
    "def concat_reps(rep_list):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_reps = np.max([r.shape[0] for r in rep_list])\n",
    "    resized_reps = []\n",
    "    for r in rep_list:\n",
    "        tmp = np.zeros((max_reps, r.shape[1], r.shape[2]))\n",
    "        tmp[...] = np.NaN\n",
    "        tmp[:r.shape[0], :, :] = r\n",
    "        resized_reps.append(tmp)\n",
    "    return np.concatenate(resized_reps, axis=-1)\n",
    "\n",
    "def fix_nan_reps(reps):\n",
    "    \"\"\"Some of the entries in neural reps might be nan. \n",
    "    Substitute those values by the average response of \n",
    "    corresponding neurons to all images over all valid reps.\n",
    "    reps = [n_reps, n_samples, n_neurons]\n",
    "    \"\"\"\n",
    "    if np.any(np.isnan(reps)):\n",
    "        # find the indices of nan neurons\n",
    "        nan_ind = np.isnan(reps)\n",
    "        _, _, nan_neu_ind = np.nonzero(nan_ind)\n",
    "\n",
    "        corrected_reps = reps\n",
    "        for n in np.unique(nan_neu_ind):\n",
    "            # create a mask of all nan values for a neuron\n",
    "            mask = np.zeros(shape=nan_ind.shape, dtype=bool)\n",
    "            mask[:, :, n] = True\n",
    "            masked_nan_ind = nan_ind & mask\n",
    "\n",
    "            # substitue all nan values of neuron by average neuron response\n",
    "            av_neuron_act = np.nanmean(reps[:, :, n])\n",
    "            corrected_reps[masked_nan_ind] = av_neuron_act\n",
    "        return corrected_reps\n",
    "    else:\n",
    "        return reps\n",
    "\n",
    "def project_reps(input_reps, W_mat):\n",
    "    \"\"\"Project each rep of neural data using the projection matrix\n",
    "    input_reps = [n_reps, n_samples, n_neurons]\"\"\"\n",
    "    input_reps = fix_nan_reps(input_reps)\n",
    "    reps = []\n",
    "    for rep in input_reps:\n",
    "        reps.append(scale(rep))\n",
    "    comp_reps = np.tensordot(reps, W_mat, axes=1)\n",
    "    return comp_reps\n",
    "\n",
    "def fit_reg(X, Y):\n",
    "    \"\"\"\n",
    "    Fits a linear regression model to the data and returns regression model with score and predictions\"\"\"\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, Y)\n",
    "    preds = reg.predict(X)\n",
    "    score = pearsonr(Y, preds)\n",
    "    return reg, score, preds\n",
    "\n",
    "def predict_outputs(features, weights):\n",
    "    \"\"\"\n",
    "    Predict outputs given input features and weights.\"\"\"\n",
    "    model_pcs = np.matmul(features - weights['pca_b'], weights['pca_w'])\n",
    "    preds = np.matmul(model_pcs, weights['pls_w']) + weights['pls_b'] \n",
    "    return preds\n",
    "\n",
    "\n",
    "def resize_mat(mat, new_size):\n",
    "    \"\"\"\n",
    "    Resize a matrix to the desired size. Input size is [num_channels, num_pixels, num_pixels]\"\"\"\n",
    "    if mat.ndim==2:\n",
    "        mat = np.expand_dims(mat, axis=0)\n",
    "    num_ch, _, num_pix = np.array(mat).shape\n",
    "\n",
    "    x = np.arange(0, num_pix)\n",
    "    y = np.arange(0, num_pix)\n",
    "    ratio = (new_size - 1.) / (num_pix - 1)\n",
    "\n",
    "    x_new = np.arange(0, new_size)\n",
    "    y_new = np.arange(0, new_size)\n",
    "\n",
    "    output = []\n",
    "    for i in range(num_ch):\n",
    "        resized_rf_func = interp2d(x * ratio, y * ratio, mat[i], kind='cubic')\n",
    "        tmp_out = resized_rf_func(x_new, y_new)\n",
    "        output.append(tmp_out)\n",
    "\n",
    "    return np.squeeze(output)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Left Magneto data and ID lookup dictionary (corresponding ids from hvm 5760 images)\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat('/braintree/home/bashivan/dropbox/Data/Ko_data/V4_data/mag_left_v4.mat')\n",
    "images_lookup = loadmat('/braintree/home/bashivan/dropbox/Data/Ko_data/id_object_lookup.mat')['id_object_lookup']\n",
    "# site_locations = loadmat('/braintree/home/bashivan/dropbox/Data/Ko_data/V4_data/kk_2_pb_sites.mat')\n",
    "\n",
    "# Find matching image ids on HVM\n",
    "ids = []\n",
    "for r in images_lookup:\n",
    "    image_num = int(np.nonzero(meta_hvm['id'] == r[1])[0])\n",
    "    ids.append(image_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separable (Mask-Mix) Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF implementation of RF limited Regression\n",
    "\n",
    "class SeparableMap(object):\n",
    "    def __init__(self, graph=None, rf_init=None, num_neurons=65, batch_size=50, init_lr=0.01,\n",
    "                ls=0.05, ld=0.1, tol=1e-2, max_epochs=10, map_type='linreg', init_rfs=None):\n",
    "        self.ld = ld    # reg factor for depth conv\n",
    "        self.ls = ls    # reg factor for spatial conv\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neurons = num_neurons\n",
    "        self.lr = init_lr\n",
    "        self._lr_ph = tf.placeholder(dtype=tf.float32)\n",
    "        self.max_epochs = max_epochs\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self._lr_ph)\n",
    "        self.map_type = map_type\n",
    "        self.init_rfs = init_rfs\n",
    "        \n",
    "        if graph is None:\n",
    "            self.graph = tf.Graph().as_default()\n",
    "        else:\n",
    "            self.graph = graph\n",
    "            \n",
    "    def iterate_minibatches(self, inputs, targets=None, batchsize=128, shuffle=False):\n",
    "        input_len = inputs.shape[0]\n",
    "\n",
    "        if shuffle:\n",
    "            indices = np.arange(input_len)\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, input_len, batchsize):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batchsize]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            if targets is None:\n",
    "                yield inputs[excerpt]\n",
    "            else:\n",
    "                yield inputs[excerpt], targets[excerpt] \n",
    "        \n",
    "    def make_separable_map(self):\n",
    "        with tf.variable_scope('mapping'):\n",
    "            if self.map_type == 'separable':\n",
    "                input_shape = self.input_ph.shape\n",
    "                preds = []\n",
    "                for n in range(self.num_neurons):\n",
    "                    with tf.variable_scope('N_{}'.format(n)):\n",
    "                        if self.init_rfs is None:\n",
    "                            s_w = tf.Variable(initial_value=np.random.randn(1, input_shape[1], input_shape[2], 1), dtype=tf.float32)\n",
    "                        else:\n",
    "                            assert self.init_rfs.shape == (self.num_neurons, input_shape[1], input_shape[2]), \\\n",
    "                            'Filter initialization matrix should be ({},{},{})'.format(self.num_neurons, input_shape[1], input_shape[2])\n",
    "                            s_w = tf.Variable(initial_value=self.init_rfs[n].reshape((1, input_shape[1], input_shape[2], 1)), dtype=tf.float32)\n",
    "                        tf.add_to_collection('s_w', s_w)\n",
    "                        out = self.input_ph * s_w\n",
    "                        d_w = tf.Variable(initial_value=np.random.randn(1, 1, out.shape[-1], 1), dtype=tf.float32)\n",
    "                        tf.add_to_collection('d_w', d_w)\n",
    "                        out = tf.nn.conv2d(out, d_w, [1, 1, 1, 1], 'SAME')\n",
    "                        bias = tf.Variable(initial_value=np.zeros((1, 1, 1, 1)), dtype=tf.float32)\n",
    "                        preds.append(tf.reduce_sum(out, axis=[1, 2]) + bias)\n",
    "                self._predictions = tf.concat(preds, -1)\n",
    "            elif self.map_type == 'linreg':\n",
    "                # For L1-Regression\n",
    "                tmp = tf.layers.flatten(self.input_ph)\n",
    "                self._predictions = tf.layers.dense(tmp, self.num_neurons)\n",
    "            \n",
    "    def make_loss(self):    \n",
    "        with tf.variable_scope('loss'):\n",
    "            self.l2_error = tf.norm(self._predictions - self.target_ph, ord=2) #  tf.reduce_sum(tf.pow(self._predictions-self.target_ph, 2))/(2*self.batch_size) # \n",
    "\n",
    "            # For L1-Regression\n",
    "            if self.map_type == 'linreg':\n",
    "                self.reg_loss = tf.reduce_sum([tf.reduce_sum(tf.abs(t)) for t in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)])\n",
    "                self.total_loss = self.l2_error + self.reg_loss\n",
    "                \n",
    "            elif self.map_type == 'separable':\n",
    "                # For separable mapping\n",
    "                self.s_vars = tf.get_collection('s_w')\n",
    "                self.d_vars = tf.get_collection('d_w')\n",
    "\n",
    "                # L1 reg\n",
    "#                 self.reg_loss = self.ls * tf.reduce_sum([tf.reduce_sum(tf.abs(t)) for t in self.s_vars]) + self.ld * tf.reduce_sum([tf.reduce_sum(tf.abs(t)) for t in self.d_vars])\n",
    "                # L2 reg\n",
    "#                 self.reg_loss = self.ls * tf.reduce_sum([tf.reduce_sum(tf.pow(t, 2)) for t in self.s_vars]) + self.ld * tf.reduce_sum([tf.reduce_sum(tf.pow(t, 2)) for t in self.d_vars])\n",
    "#                 self.total_loss = self.l2_error + self.reg_loss\n",
    "\n",
    "                # Laplacian loss\n",
    "                laplace_filter = tf.constant(np.array([0, -1, 0, -1, 4, -1, 0, -1, 0]).reshape((3, 3, 1, 1)), dtype=tf.float32)\n",
    "                laplace_loss = tf.reduce_sum([tf.norm(tf.nn.conv2d(t, laplace_filter, [1, 1, 1, 1], 'SAME')) for t in self.s_vars])\n",
    "                l2_loss = tf.reduce_sum([tf.reduce_sum(tf.pow(t, 2)) for t in self.s_vars])\n",
    "                self.reg_loss = self.ls * (l2_loss + laplace_loss) + \\\n",
    "                                self.ld * tf.reduce_sum([tf.reduce_sum(tf.pow(t, 2)) for t in self.d_vars])\n",
    "\n",
    "                self.total_loss = self.l2_error + self.reg_loss\n",
    "            self.tvars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            self.train_op = self.opt.minimize(self.total_loss, var_list=self.tvars, \n",
    "                                             global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        if self.map_type == 'linreg':\n",
    "            assert X.ndim == 2, 'Input matrix rank should be 2.'\n",
    "        else:\n",
    "            assert X.ndim == 4, 'Input matrix rank should be 4.'\n",
    "        self.input_ph = tf.placeholder(dtype=tf.float32, shape=[None]+list(X.shape[1:]))\n",
    "        self.target_ph = tf.placeholder(dtype=tf.float32, shape=[None, Y.shape[-1]])\n",
    "        # Build the model graph\n",
    "        self.model = self.make_separable_map()\n",
    "        self.make_loss()\n",
    "        \n",
    "        # initialize graph\n",
    "        print('Initializing...')\n",
    "        self.sess = tf.Session()\n",
    "        init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "        self.sess.run(init_op)\n",
    "        for e in range(self.max_epochs):\n",
    "            for counter, batch in enumerate(self.iterate_minibatches(X, Y, batchsize=self.batch_size, shuffle=True)):\n",
    "                feed_dict = {self.input_ph: batch[0], \n",
    "                             self.target_ph: batch[1],\n",
    "                            self._lr_ph: self.lr}\n",
    "                _, loss_value, reg_loss_value = self.sess.run([self.train_op, self.l2_error, self.reg_loss], feed_dict=feed_dict)\n",
    "            if e % 100 == 0:\n",
    "                print('Epoch: %d, Err Loss: %.2f, Reg Loss: %.2f' % (e+1, loss_value, reg_loss_value))\n",
    "            if e % 200 == 0 and e != 0:\n",
    "                self.lr /= 10.\n",
    "            if loss_value < self.tol:\n",
    "                print('Converged.')\n",
    "                break\n",
    "            \n",
    "                \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for batch in self.iterate_minibatches(X, batchsize=self.batch_size, shuffle=False):\n",
    "            feed_dict = {self.input_ph: batch}\n",
    "            preds.append(np.squeeze(self.sess.run([self._predictions], feed_dict=feed_dict)))\n",
    "        return np.concatenate(preds, axis=0)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import h5py\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "map_type = 'separable'\n",
    "\n",
    "\n",
    "# feats = np.array(h5py.File('/braintree/data2/active/users/bashivan/model_features/alexnet_model_139k.hdf5')['mdl_conv3'])[ids]\n",
    "# feats = feats.reshape(-1, 17, 17, 384)\n",
    "\n",
    "# n_comps = 1000\n",
    "# pca = PCA(n_components=n_comps)\n",
    "# pca_feats = pca.fit_transform(feats)\n",
    "\n",
    "# v4_data_reps = np.array(h5py.File('/braintree/home/bashivan/dropbox/Data/Ko_data/V4_data/season5/hvm_initial_new_normalized_rates_kk_pb.mat')['normalized_rates'])\n",
    "# v4_data_reps = v4_data_reps.transpose(2, 0, 1)\n",
    "# neurons = np.nanmean(v4_data_reps, axis=0)\n",
    "\n",
    "# Load RF data\n",
    "v4_rf_cells = np.array(h5py.File('/braintree/home/bashivan/dropbox/Data/Ko_data/V4_data/rf_mask_perCell.mat')['rf_mask_percell'])\n",
    "v4_rf_cells = v4_rf_cells.transpose(2, 0, 1)\n",
    "rf_inits = resize_mat(v4_rf_cells, 17)\n",
    "\n",
    "if map_type == 'linreg':\n",
    "    X = pca_feats\n",
    "else:\n",
    "    X = feats\n",
    "Y = scale(neurons)\n",
    "    \n",
    "train_ind = np.random.choice(range(X.shape[0]), 576, replace=False)\n",
    "test_ind = np.nonzero(~ np.in1d(range(X.shape[0]), train_ind))[0]\n",
    "\n",
    "all_scores = np.zeros((3, 3))\n",
    "\n",
    "for ls_ind, ls in enumerate([1, 2, 5]):  # [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "    for ld_ind, ld in enumerate([1, 2, 5]):\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            with tf.Session() as sess:\n",
    "                print 'Mapping with ls={}, ld={}'.format(ls, ld)\n",
    "                mapper = SeparableMap(graph=graph, max_epochs=1000, tol=0.1, \n",
    "                                      init_lr=0.01, batch_size=50, ls=ls, ld=ld, \n",
    "                                      map_type=map_type, init_rfs=rf_inits)\n",
    "                mapper.fit(X[train_ind], Y[train_ind])\n",
    "                preds = mapper.predict(X[test_ind])\n",
    "                scores = [pearsonr(preds[:, i], Y[test_ind, i])[0] for i in range(preds.shape[-1])]\n",
    "                all_scores[ls_ind, ld_ind] = np.median(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4.0,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
